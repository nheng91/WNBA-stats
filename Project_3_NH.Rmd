---
title: "Project 3"
author: "Nate Heng"
date: 'Last updated: `r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=8)
```

```{r, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(VIM)
library(moderndive)
library(broom)
library(kableExtra)
library(catspec)
library(olsrr)
library(MASS)
library(car)
library(lindia)
library(lmtest)

### almost_sas function from class
almost_sas <- function(aov.results){
  aov_residuals <- residuals(aov.results)
  par(mfrow=c(2,2))
  plot(aov.results, which=1)
  hist(aov_residuals)
  plot(aov.results, which=2)
  plot(density(aov_residuals))
}
### p.value.string v2
# by Reid Ginoza
p.value.string = function(p.value){
  p.value <- round(p.value, digits=4)
  if (p.value == 0) {
    return("p < 0.0001")
  } else {
    return(paste0("p = ", format(p.value, scientific = F)))
  }
}
```

# Data

The dataset I am using for the below analysis is from the [WNBA-stats](https://github.com/fivethirtyeight/WNBA-stats) repository maintained by [FiveThiryEight](https://github.com/fivethirtyeight), specifically the [WNBA Player Stats csv](https://github.com/fivethirtyeight/WNBA-stats/blob/master/wnba-player-stats.csv).  The data set contains statistics (various shooting percentages, assists, rebounds, etc.) on players for each season between 1997 and 2019.  The goal of this analysis will be to determine if a subset of the variables found within the data set is predictive of a player's **'Player Efficiency Rating'** or **(PER)**.  Player Efficiency Rating is an overall measure of a player's production and encompasses specific statistics like shooting percentages, assists, turnovers, and others within its [calculation](https://en.wikipedia.org/wiki/Player_efficiency_rating).  Since **PER** compounds several of a player's on-court statistics into one overall statistic I will look at other potential predictors that do not utilize them.  The variables that will be assessed as predictors are below:

* **Age**: Age of player during season
* **Decade**: Decade of season (2000s or 2010)
* **Court Position**: Front or Back Court. Guards are considered back court players while Forwards and Centers are considered front court players
* **Team Net Efficiency Rating**: A similar metric to **PER**, but at the team level.  The Team Net Efficiency Rating will be used as a measure of how good or bad a player's team is during a given year.

The data set was initially filtered to exclude player's who played less than 100 minutes during a season because anything under 100 minutes would not be a large enough sample size for a reliable calculation, and any age lower than the minimum allowable age by the WNBA was dropped as well.  Any seasons prior to 2000 were also dropped to allow the data set to contain two full decades for testing. The decade of a player was created to be a categorical variable placing years 2000-2009 in the '2000s' decade and years 2010-2019 in the '2010s' decade.  Court position was another created variable to decrease to 2 the number of categories a player's position could fall under. The data set had a few missing values for court position, so the missing values were imputed based on a player's rebound percentage, assist percentage, block percentage, and three point percentage.  The code to create these variables as well as confirmation of no missing values after imputation is shown below.

```{r, message=FALSE, warning=FALSE}
wnba <- read.csv("wnba-player-stats.csv")
wnba_filtered <- wnba %>% filter(year_ID >= 2000, MP > 100, Age >= 18) %>% 
  mutate(decade = case_when(
           year_ID >=2000 & year_ID < 2010 ~ "2000s",
           year_ID >=2010 ~ "2010s"), 
         court_position = case_when(
           Pos == "G" ~ "Back",
           Pos == "G-F" ~ "Back",
           Pos == "F" ~ "Front",
           Pos == "C" ~ "Front",
           Pos == "F-G" ~ "Front",
           Pos == "F-C" ~ "Front",
           Pos == "C-F" ~ "Front"),
         TRB_pct_scaled = scale(TRB_pct),
         AST_pct_scaled = scale(AST_pct),
         BLK_pct_scaled = scale(BLK_pct),
         ThrPAr_scaled = scale(ThrPAr))
wnba_imputed <- kNN(wnba_filtered, variable = "court_position", dist_var = c("TRB_pct_scaled", "AST_pct_scaled", "BLK_pct_scaled", "ThrPAr_scaled"))
wnba_final <- wnba_imputed %>% dplyr::select(PER, decade, court_position, Tm_Net_Rtg, Age)
wnba_final_na <-  wnba_final %>% is.na() %>% colSums()
knitr::kable(wnba_final_na, col.names = "", caption = "**# of NAs by Variable**") %>% kable_styling()
```

## Summary Statistics

```{r}
wnba_numeric <- wnba_final %>% select_if(is.numeric)
wnba_categorical <- wnba_final %>% select_if(is.character)
wnba_means <- round(apply(wnba_numeric, 2, mean),3)
wnba_sd <- round(apply(wnba_numeric, 2, sd),3)
wnba_median <- round(apply(wnba_numeric, 2, median),3)
wnba_min <- round(apply(wnba_numeric, 2, min),3)
wnba_max <- round(apply(wnba_numeric, 2, max),3)
wnba_numeric_summary <- data.frame(cbind(c("Player Efficiency Rating", "Team Net Efficiency Rating", "Age"), unname(wnba_means), unname(wnba_median), unname(wnba_sd), unname(wnba_min), unname(wnba_max)))
colnames(wnba_numeric_summary) <- c("Variable", "Mean", "Median", "Standard Deviation", "Min", "Max")
```

Summary Statistics (Mean, Median, Standard Deviation, Min, Max) for the continuous variables (PER, Team Net Rating, Age) are shown in the below table.

```{r}
knitr::kable(wnba_numeric_summary, caption = "**Continuous Predictors**") %>% kable_styling(full_width = F, position = "left")
```

Each variable has a mean that is very close to its median indicating a lack of skewness within the data.  PER has a mean/median of 13.482/13 and standard deviation of 5.426 which suggests most of the values lie between 2 and 24 with some outliers of extremely good and extremely bad players.  Team Net Rating has a mean of -.222/.2 and standard deviation of 6.028 which suggests most of the values lie between -12 and 12 with some outliers for extremely good and extremely bad teams. Age has a mean of 26.556/26 and standard deviation of 3.96 which suggests most of the values lie between 19 and 34 with outliers of some older players.

Frequency Tables for both categorical variables (Decade and Court Position) are shown below.  

```{r}
decade_tab <- ctab(table(wnba_categorical$decade))
decade_df <- data.frame(rownames(decade_tab$ctab), unname(decade_tab$ctab[,1]), round(unname(decade_tab$ctab[,2]),1))
cp_tab <- ctab(table(wnba_categorical$court_position))
cp_df <- data.frame(rownames(cp_tab$ctab), unname(cp_tab$ctab[,1]), round(unname(cp_tab$ctab[,2]),1))
knitr::kable(decade_df, col.names = c("Decade", "Frequency", "Percentage of Observations")) %>% kable_styling(full_width = F, position = "left")
knitr::kable(cp_df, col.names = c("Court Position", "Frequency", "Percentage of Observations")) %>% kable_styling(full_width = F, position = "left")
```

Each variable is relatively balanced between levels.

# Model Comparison

The analysis will compare two models using the model evaluation criterion of $AIC$ and $R_{adj}^2$. The two models being compared are described below. 

## Full Model

The full model uses decade, court position, team net rating, player age and an interaction between decade and court position to estimate a player's efficiency rating.  The 2000s decade will be the base level for the decade variable, and Back will be the base level for the court position variable.  This will persist throughout the analysis.  This means the $X_{Decade}$ variable will be 1 when the decade is 2010s and 0 when the decade is 2000s, and the $X_{CourtPosition}$ variable will be 1 when the court position is Front and 0 when the court position is Back.

The estimated model is as follows:

$\hat{Y_{\text{i,PER}}} = b_0 + b_{\text{Decade}}X_{\text{i,Decade}} + b_{\text{CourtPosition}}X_{\text{i,CourtPosition}} + b_{\text{TeamNetRating}}X_{\text{i,TeamNetRating}} + b_{\text{Age}}X_{\text{i,Age}}  + b_{\text{Decade x CourtPosition}}X_{\text{i,Decade}}X_{\text{i,CourtPosition}}$

A summary of the full multiple linear regression model and the resulting model fit are given below.

```{r}
wnba_full_mod <- lm(PER ~ decade*court_position + Tm_Net_Rtg + Age, data = wnba_final)
coef_full <- tidy(wnba_full_mod)$estimate
summary(wnba_full_mod)
```
**Full Model Estimate:**

$\hat{Y_{\text{i,PER}}} = `r round(coef_full[1], digits=2)` - `r -1*round(coef_full[2], digits=2)`X_{\text{i,Decade}} + `r round(coef_full[3], digits=2)`X_{\text{i,CourtPosition}} + `r round(coef_full[4], digits=2)`X_{\text{i,TeamNetRating}} + `r round(coef_full[5], digits=2)`X_{\text{i,Age}}  + `r round(coef_full[6], digits=2)`X_{\text{i,Decade}}X_{\text{i,CourtPosition}}$

## Reduced Model

The reduced model will drop the interaction term between decade and court position. The reduced form of the model that does not include the interaction term is shown below.

$\hat{Y_{\text{i,PER}}} = b_0 + b_{\text{Decade}}X_{\text{i,Decade}} + b_{\text{CourtPosition}}X_{\text{i,CourtPosition}} + b_{\text{TeamNetRating}}X_{\text{i,TeamNetRating}} + b_{\text{Age}}X_{\text{i,Age}}$

A summary of the reduced multiple linear regression model (**Reduced Model**) and the resulting model fit are given below.

```{r}
wnba_reduced_mod <- lm(PER ~ decade + court_position + Tm_Net_Rtg + Age, data = wnba_final)
coef_red1 <- tidy(wnba_reduced_mod)$estimate
summary(wnba_reduced_mod)
```

**Reduced Model #1 Estimate:**

$\hat{Y_{\text{i,PER}}} = `r round(coef_red1[1], digits=2)` + `r round(coef_red1[2], digits=2)`X_{\text{i,Decade}} + `r round(coef_red1[3], digits=2)`X_{\text{i,CourtPosition}} + `r round(coef_red1[4], digits=2)`X_{\text{i,TeamNetRating}} + `r round(coef_red1[5], digits=2)`X_{\text{i,Age}}$


## Model Comparison Statistics

A table containing both $AIC$ and $R_{adj}^2$ values for each model is shown below.

```{r}
Model_Name <- c("Full Model", "Reduced Model")
AIC_mod <- c(ols_aic(wnba_full_mod, method = "SAS"), ols_aic(wnba_reduced_mod, method = "SAS"))
Adj_r2 <- c(summary(wnba_full_mod)$adj.r.squared, summary(wnba_reduced_mod)$adj.r.squared)
model_df <- data.frame(Model_Name, AIC_mod, Adj_r2)
knitr::kable(model_df, col.names = c("Model", "AIC", "Adjusted R-Squared")) %>% kable_styling(full_width = F, position = "left")
```

Both the $AIC$ value and the $R_{adj}^2$ value for the full model are slightly better than for the model without the interaction term, so both criterion values are in agreement that the full model would be considered a better fit to the data.  This is in opposition to the results of project 2 where the interaction term was dropped in favor of the reduced model as a result of an F-test comparing the two models.  Based on the results of the above model comparison statistics, the **Full Model** will be considered the "best model" going forward. 

# Model Diagnostics

Using the full model, I will attempt to identify any outliers and influential points within the data, whether or not the model contains multicollinearity, and whether or not the model meets the appropriate assumptions associated with a linear regression. 

## Outliers

An observation can be an outlier in either the predictor variable, response variable, or both variables.

### Outliers in Y

The below code seeks to find any outliers in the response variable, **PER**.

```{r}
student_res <- studres(wnba_full_mod)
outlier_y_flag <- as.factor(ifelse(abs(student_res) > 3, "Outlier", "Normal"))
student_res_df <- data.frame("obs" = 1:length(student_res), student_res, outlier_y_flag)
student_res_outlier_df <- student_res_df %>% filter(outlier_y_flag == "Outlier")
knitr::kable(table(outlier_y_flag), col.names = c("Obs. Type", "Freq")) %>% kable_styling(full_width = F, position = "left")
ggplot(student_res_df, aes(x = obs, y = student_res, col = outlier_y_flag)) + 
  geom_point() + 
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  geom_text(data = student_res_outlier_df, aes(obs+70, student_res, label = obs), show.legend = FALSE) +
  scale_color_manual(values = c("black", "red")) + 
  scale_y_continuous(breaks=seq(-4,4,by=1)) + 
  labs(x = "Observation", y = "Deleted Student Residual", color = "Observation Type") +
  theme_classic()
```

16 observations are identified as potential outliers in the response variable.  They are shown in red in the above graph.

### Outliers in X

The below code seeks to find any outliers in the predictor variables.

```{r}
p <- length(tidy(wnba_full_mod)$estimate)
n <- nrow(wnba_final)
outlier_x_cutoff <- (2*p)/n
hat_values <- hatvalues(wnba_full_mod)
outlier_x_flag <- as.factor(ifelse(abs(hat_values) > outlier_x_cutoff, "Outlier", "Normal"))
hat_df <- data.frame("obs" = 1:length(hat_values), hat_values, outlier_x_flag)
hat_outlier_df <- hat_df %>% filter(outlier_x_flag == "Outlier")
knitr::kable(table(outlier_x_flag), col.names = c("Obs. Type", "Freq")) %>% kable_styling(full_width = F, position = "left")
ggplot(hat_df, aes(x = obs, y = hat_values, col = outlier_x_flag)) + 
  geom_point() + 
  geom_hline(yintercept = outlier_x_cutoff, color = "red") +
  geom_text(data = hat_outlier_df, aes(obs+70, hat_values, label = obs), show.legend = FALSE) +
  scale_color_manual(values = c("black", "red")) + 
  scale_y_continuous(breaks=round(c(0,0.0025,outlier_x_cutoff,0.005,0.0075,0.01),4)) + 
  labs(x = "Observation", y = "Hat Value", color = "Observation Type") +
  theme_classic()
```

67 observations are identified as potential outliers in the predictor variables.  They are shown in red in the above graph.

### Outliers in Both X and Y

The below code seeks to discover the intersection of predictor and response outliers.

```{r}
outlier_df <- data.frame("obs" = 1:length(hat_values), outlier_x_flag, outlier_y_flag)
outliers_x_y <- outlier_df %>% filter(outlier_x_flag == "Outlier",  outlier_y_flag == "Outlier") %>% pull(obs)
both_outliers <- length(outliers_x_y)
names(both_outliers) <- "Number of Outliers in Both X & Y"
knitr::kable(length(outliers_x_y), col.names = "Number of Outliers in Both X & Y") %>% kable_styling(full_width = F, position = "left")
```

The above table shows that there are no observations that are ooutliers in both the predictors and the response.

### Points of Influence/Leverage

The below code seeks to determine and points of influence based on Cook's Distance.  Influential points can have a disproportiante efect on the model estimate.

```{r}
cooksd_threshold <- 4/n
cooksd <- cooks.distance(wnba_full_mod)
influential_flag <- as.factor(ifelse(abs(cooksd) > cooksd_threshold, "Influential", "Normal"))
knitr::kable(table(influential_flag), col.names = c("Obs. Type", "Freq")) %>% kable_styling(full_width = F, position = "left")
gg_cooksd(wnba_full_mod) + theme_classic()
```

145 observations are identified as potentially influential.  They are shown in red in the above graph.

### Observations that are Outliers in both X and Y and Influential

There are a high number of points observed to be outliers in the predictor variables, outliers in the response variable, or influential.
The below code seeks to discover the overlap between outliers and influential points.  We already know there are no points that are outliers in both predictor and response.

```{r}
observation_class_df <- data.frame("obs" = 1:length(hat_values), outlier_y_flag, outlier_x_flag, influential_flag)
points_of_interest_x <- observation_class_df %>% filter(outlier_x_flag == "Outlier", influential_flag == "Influential") %>% pull(obs)
knitr::kable(length(points_of_interest_x), col.names = "Number of Outliers in X that are also Influential") %>% kable_styling(full_width = F, position = "left")
points_of_interest_y <- observation_class_df %>% filter(outlier_y_flag == "Outlier", influential_flag == "Influential") %>% pull(obs)
knitr::kable(length(points_of_interest_y), col.names = "Number of Outliers in Y that are also Influential") %>% kable_styling(full_width = F, position = "left")
points_of_interest <- length(points_of_interest_y)+length(points_of_interest_x)
knitr::kable(points_of_interest, col.names = "Number of Outliers that are also Influential") %>% kable_styling(full_width = F, position = "left")
```

There appears to be 31 total points of interest.  16 are outliers in the response variable, and 15 are outliers in the predictor variables.

## Multicollinearity

We cannot include the interaction term when measuring collinearity, so the **Reduced Model** will be used to measure the variance inflation factors (vif) for each variable.

```{r}
vif(wnba_reduced_mod)
```

The vif value for each predictor is not considerably larger than 1 so none will be dropped from the model.

## Regression Assumptions


```{r}
almost_sas(wnba_full_mod)
```

The normality assumptions of the residuals appears to hold.  The histogram and density plots look to be symmetrical and bellshaped around 0, and the qq-plot appears to fall on the 45 degree line, with a few outliers in the tails.  However, in the first residual plot there appears to maybe be some widening of the variance as the fitted values increase, but it is not clear.  I will use Breusch-Pagan tests to test for constancy of error variance on the continuous variables.  $\alpha = 0.05$ will be used to test for significance.  The hypotheses are as follows:

### Hypotheses Tests for Constancy of Error Variance

**Age**

$H_0: \ \gamma_{\text{Age}} = 0$ \
$H_1: \ \gamma_{\text{Age}} \ne 0$ \

The results of the Breusch-Pagan test for age are shown below.

```{r}
wnba_Age <- lm(PER ~ Age, data = wnba_final)
(bp_age <- bptest(wnba_Age, studentize = FALSE))
```
**Test Statistic**

$\chi_0^2 = `r round(unname(bp_age$statistic), digits=4)`$

***p*-value**

$`r p.value.string(unname(bp_age$p.value))`$

**Rejection Region**

Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

**Conclusion and Interpretation**

Fail to reject $H_0$. There is not sufficient evidence to suggest that the variance assumption is broken for age as a predictor.  Age is safe to leave as is in the **Full Model**.

**Team Net Rating**

$H_0: \ \gamma_{\text{Team Net Rating}} = 0$ \
$H_1: \ \gamma_{\text{Team Net Rating}} \ne 0$ \

The results of the Breusch-Pagan test for team net rating are shown below.

```{r}
wnba_TNR <- lm(PER ~ Tm_Net_Rtg, data = wnba_final)
(bp_TNR <- bptest(wnba_TNR, studentize = FALSE))
```

**Test Statistic**

$\chi_0^2 = `r round(unname(bp_TNR$statistic), digits=4)`$

***p*-value**

$`r p.value.string(unname(bp_TNR$p.value))`$

**Rejection Region**

Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

**Conclusion and Interpretation**

Reject $H_0$. There is sufficient evidence to suggest that the variance assumption is broken for team net rating as a predictor.  A transformation to team net rating may need to be considered.

## Next Steps


# Conclusion / Interpretation




